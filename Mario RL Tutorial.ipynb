{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab에서 노트북을 실행하실 때에는 \n",
        "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ENV setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## uv 설치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<url>https://sigridjin.medium.com/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EA%B0%9C%EB%B0%9C%EC%9E%90%EB%9D%BC%EB%A9%B4-uv-%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%A9%EC%8B%9C%EB%8B%A4-546d523f7178<url>\n",
        "<br>\n",
        "<url>https://rudaks.tistory.com/entry/python%EC%9D%98-uv-%EC%82%AC%EC%9A%A9%EB%B2%95<url>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (3163135473.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    brew install uv\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# terminal 에서\n",
        "brew install uv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# project 생성 : 폴더 생성하니 프로젝트를 만들고 싶은 위치에서 실행\n",
        "uv init <project_name> \n",
        "# 설치가 되면 디렉토리 이동\n",
        "cd <project_name>\n",
        "\n",
        "# venv 생성\n",
        "uv venv <venv_name>\n",
        "# venv 실행\n",
        "source ./venv/bin/activate\n",
        "# 파이썬 설치\n",
        "uv python install 3.10 or later\n",
        "# 주의!!! ipynb 사용 시에는 반드시 --dev를 붙여서 ipykernel 설치 할 것. 파이썬 버전은 이 라이브러리 버전으로 잡히니, notebook에서 버전 확인이 이상하면 이 패키지 관리\n",
        "uv add --dev ipykernel\n",
        "# 나머지 필요 패키지 설치\n",
        "uv add torch torchvision torchaudio gym-super-mario-bros numpy=1.22.4 pandas matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error uint8~~~\n",
        "<url>https://stackoverflow.com/questions/78757000/overflowerror-when-setting-up-gym-super-mario-bros-environment-in-python-on-jupy<url>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 오래된 버전이기도 하고 uint8을 쓰면 경량화 효과는 있지만, float32로 키워보자,,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "마리오 게임 RL 에이전트로 학습하기\n",
        "==================================\n",
        "\n",
        "**Authors**: [Yuansong Feng](https://github.com/YuansongFeng), [Suraj Subramanian](https://github.com/suraj813), [Howard Wang](https://github.com/hw26), [Steven Guo](https://github.com/GuoYuzhang).\n",
        "\n",
        ":   **번역**: [김태영](https://github.com/Taeyoung96).\n",
        "\n",
        "이번 튜토리얼에서는 심층 강화 학습의 기본 사항들에 대해 이야기해보도록\n",
        "하겠습니다. 마지막에는, 스스로 게임을 할 수 있는 AI 기반 마리오를\n",
        "([Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf) 사용)\n",
        "구현하게 됩니다.\n",
        "\n",
        "이 튜토리얼에서는 RL에 대한 사전 지식이 필요하지 않지만, 이러한\n",
        "[링크](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
        "를 통해 RL 개념에 친숙해 질 수 있으며, 여기 있는\n",
        "[치트시트](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N)\n",
        "를 활용할 수도 있습니다. 튜토리얼에서 사용하는 전체 코드는\n",
        "[여기](https://github.com/yuansongFeng/MadMario/) 에서 확인 할 수\n",
        "있습니다.\n",
        "\n",
        "![](https://tutorials.pytorch.kr/_static/img/mario.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` {.sourceCode .bash}\n",
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0\n",
        "pip install tensordict==0.3.0\n",
        "pip install torchrl==0.3.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os\n",
        "\n",
        "# Gym은 강화학습을 위한 OpenAI 툴킷입니다.\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# OpenAI Gym을 위한 NES 에뮬레이터\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# OpenAI Gym에서의 슈퍼 마리오 환경 세팅\n",
        "import gym_super_mario_bros\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "강화학습 개념\n",
        "=============\n",
        "\n",
        "**환경(Environment)** : 에이전트가 상호작용하며 스스로 배우는\n",
        "세계입니다.\n",
        "\n",
        "**행동(Action)** $a$ : 에이전트가 환경에 어떻게 응답하는지 행동을 통해\n",
        "나타냅니다. 가능한 모든 행동의 집합을 *행동 공간* 이라고 합니다.\n",
        "\n",
        "**상태(State)** $s$ : 환경의 현재 특성을 상태를 통해 나타냅니다. 환경이\n",
        "있을 수 있는 모든 가능한 상태 집합을 *상태 공간* 이라고 합니다.\n",
        "\n",
        "**포상(Reward)** $r$ : 포상은 환경에서 에이전트로 전달되는 핵심\n",
        "피드백입니다. 에이전트가 학습하고 향후 행동을 변경하도록 유도하는\n",
        "것입니다. 여러 시간 단계에 걸친 포상의 합을 **리턴(Return)** 이라고\n",
        "합니다.\n",
        "\n",
        "**최적의 행동-가치 함수(Action-Value function)** $Q^*(s,a)$ : 상태 $s$\n",
        "에서 시작하면 예상되는 리턴을 반환하고, 임의의 행동 $a$ 를 선택합니다.\n",
        "그리고 각각의 미래의 단계에서 포상의 합을 극대화하는 행동을 선택하도록\n",
        "합니다. $Q$ 는 상태에서 행동의 \"품질\" 을 나타냅니다. 우리는 이 함수를\n",
        "근사 시키려고 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "환경(Environment)\n",
        "=================\n",
        "\n",
        "환경 초기화하기\n",
        "---------------\n",
        "\n",
        "마리오 게임에서 환경은 튜브, 버섯, 그 이외 다른 여러 요소들로 구성되어\n",
        "있습니다.\n",
        "\n",
        "마리오가 행동을 취하면, 환경은 변경된 (다음)상태, 포상 그리고 다른\n",
        "정보들로 응답합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### detail setting\n",
        "<br/> SuperMarioBros-\"world\"-\"stage\"-v\"version\"\n",
        "<br>\"world\" is a number in {1, 2, 3, 4, 5, 6, 7, 8} indicating the world\n",
        "<br>\"stage\" is a number in {1, 2, 3, 4} indicating the stage within a world\n",
        "<br>\"version\" is a number in {0, 1, 2, 3} specifying the ROM mode to use\n",
        "\n",
        "<br>0: standard ROM\n",
        "<br>1: downsampled ROM\n",
        "<br>2: pixel ROM\n",
        "<br>3: rectangle ROM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bagjuhyeon/Documents/WorkSpace/RL-game/.venv/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/Users/bagjuhyeon/Documents/WorkSpace/RL-game/.venv/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        }
      ],
      "source": [
        "# 슈퍼 마리오 환경 초기화하기 (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "# numpy 2.0 version은 uint8에서 crash\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='human', apply_api_compatibility=True)\n",
        "\n",
        "# 상태 공간을 2가지로 제한하기\n",
        "#   0. 오른쪽으로 걷기\n",
        "#   1. 오른쪽으로 점프하기\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "환경 전처리 과정 거치기\n",
        "=======================\n",
        "\n",
        "`다음 상태(next_state)` 에서 환경 데이터가 에이전트로 반환됩니다. 앞서\n",
        "살펴보았듯이, 각각의 상태는 `[3, 240, 256]` 의 배열로 나타내고 있습니다.\n",
        "종종 상태가 제공하는 것은 에이전트가 필요로 하는 것보다 더 많은\n",
        "정보입니다. 예를 들어, 마리오의 행동은 파이프의 색깔이나 하늘의 색깔에\n",
        "좌우되지 않습니다!\n",
        "\n",
        "아래에 설명할 클래스들은 환경 데이터를 에이전트에 보내기 전 단계에서\n",
        "전처리 과정에 사용할 **래퍼(Wrappers)** 입니다.\n",
        "\n",
        "`GrayScaleObservation` 은 RGB 이미지를 흑백 이미지로 바꾸는 일반적인\n",
        "래퍼입니다. `GrayScaleObservation` 클래스를 사용하면 유용한 정보를 잃지\n",
        "않고 상태의 크기를 줄일 수 있습니다. `GrayScaleObservation` 를 적용하면\n",
        "각각 상태의 크기는 `[1, 240, 256]` 이 됩니다.\n",
        "\n",
        "`ResizeObservation` 은 각각의 상태(Observation)를 정사각형 이미지로 다운\n",
        "샘플링합니다. 이 래퍼를 적용하면 각각 상태의 크기는 `[1, 84, 84]` 이\n",
        "됩니다.\n",
        "\n",
        "`SkipFrame` 은 `gym.Wrapper` 으로부터 상속을 받은 사용자 지정\n",
        "클래스이고, `step()` 함수를 구현합니다. 왜냐하면 연속되는 프레임은 큰\n",
        "차이가 없기 때문에 n개의 중간 프레임을 큰 정보의 손실 없이 건너뛸 수\n",
        "있기 때문입니다. n번째 프레임은 건너뛴 각 프레임에 걸쳐 누적된 포상을\n",
        "집계합니다.\n",
        "\n",
        "`FrameStack` 은 환경의 연속 프레임을 단일 관찰 지점으로 바꾸어 학습\n",
        "모델에 제공할 수 있는 래퍼입니다. 이렇게 하면 마리오가 착지 중이였는지\n",
        "또는 점프 중이었는지 이전 몇 프레임의 움직임 방향에 따라 확인할 수\n",
        "있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"모든 `skip` 프레임만 반환합니다.\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"행동을 반복하고 포상을 더합니다.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self._skip):\n",
        "            # 포상을 누적하고 동일한 작업을 반복합니다.\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        # dtype: uint8 -> float32: 수렴성을 위해?\n",
        "        self.observation_space = Box(low=0, high=1, shape=obs_shape, dtype=np.float32)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # [H, W, C] 배열을 [C, H, W] 텐서로 바꿉니다.\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float32)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Compose([\n",
        "            T.Grayscale(),\n",
        "        ])\n",
        "        observation = transform(observation)\n",
        "        observation /= 255.0 # Normalize 0~255 -> 0~1\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=1, shape=obs_shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 1)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# 래퍼를 환경에 적용합니다.\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "    env = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "앞서 소개한 래퍼를 환경에 적용한 후, 최종 래핑 상태는 왼쪽 아래 이미지에\n",
        "표시된 것처럼 4개의 연속된 흑백 프레임으로 구성됩니다. 마리오가 행동을\n",
        "할 때마다, 환경은 이 구조의 상태로 응답합니다. 구조는 `[4, 84, 84]`\n",
        "크기의 3차원 배열로 구성되어 있습니다.\n",
        "\n",
        "![](https://tutorials.pytorch.kr/_static/img/mario_env.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "에이전트(Agent)\n",
        "===============\n",
        "\n",
        "`Mario` 라는 클래스를 이 게임의 에이전트로 생성합니다. 마리오는 다음과\n",
        "같은 기능을 할 수 있어야 합니다.\n",
        "\n",
        "-   **행동(Act)** 은 (환경의) 현재 상태를 기반으로 최적의 행동 정책에\n",
        "    따라 정해집니다.\n",
        "-   경험을 **기억(Remember)** 하는 것. 경험은 (현재 상태, 현재 행동,\n",
        "    포상, 다음 상태) 로 이루어져 있습니다. 마리오는 그의 행동 정책을\n",
        "    업데이트 하기 위해 *캐시(caches)* 를 한 다음, 그의 경험을\n",
        "    *리콜(recalls)* 합니다.\n",
        "-   **학습(Learn)** 을 통해 시간이 지남에 따라 더 나은 행동 정책을\n",
        "    택합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"상태가 주어지면, 입실론-그리디 행동(epsilon-greedy action)을 선택해야 합니다.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"메모리에 경험을 추가합니다.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"메모리로부터 경험을 샘플링합니다.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"일련의 경험들로 실시간 행동 가치(online action value) (Q) 함수를 업데이트 합니다.\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이번 섹션에서는 마리오 클래스의 매개변수를 채우고, 마리오 클래스의\n",
        "함수들을 정의하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "행동하기(Act)\n",
        "=============\n",
        "\n",
        "주어진 상태에 대해, 에이전트는 최적의 행동을 이용할 것인지 임의의 행동을\n",
        "선택하여 분석할 것인지 선택할 수 있습니다.\n",
        "\n",
        "마리오는 임의의 행동을 선택했을 때 `self.exploration_rate` 를\n",
        "활용합니다. 최적의 행동을 이용한다고 했을 때, 그는 최적의 행동을\n",
        "수행하기 위해 (`학습하기(Learn)` 섹션에서 구현된) `MarioNet` 이\n",
        "필요합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            self.device = \"mps\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        # 마리오의 DNN은 최적의 행동을 예측합니다 - 이는 학습하기 섹션에서 구현합니다.\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # Mario Net 저장 사이의 경험 횟수\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        주어진 상태에서, 입실론-그리디 행동(epsilon-greedy action)을 선택하고, 스텝의 값을 업데이트 합니다.\n",
        "\n",
        "        입력값:\n",
        "        state (``LazyFrame``): 현재 상태에서의 단일 상태(observation)값을 말합니다. 차원은 (state_dim)입니다.\n",
        "        출력값:\n",
        "        ``action_idx`` (int): Mario가 수행할 행동을 나타내는 정수 값입니다.\n",
        "        \"\"\"\n",
        "        # 임의의 행동을 선택하기\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # 최적의 행동을 이용하기\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            state /= 255.0 # Normalize 0~255 -> 0~1\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # exploration_rate 감소하기\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # 스텝 수 증가하기\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "캐시(Cache)와 리콜(Recall)하기\n",
        "==============================\n",
        "\n",
        "이 두가지 함수는 마리오의 \"메모리\" 프로세스 역할을 합니다.\n",
        "\n",
        "`cache()`: 마리오가 행동을 할 때마다, 그는 `경험` 을 그의 메모리에\n",
        "저장합니다. 그의 경험에는 현재 *상태* 에 따른 수행된 *행동* ,\n",
        "행동으로부터 얻은 *포상* , *다음 상태*, 그리고 게임 *완료* 여부가\n",
        "포함됩니다.\n",
        "\n",
        "`recall()`: Mario는 자신의 기억에서 무작위로 일련의 경험을 샘플링하여\n",
        "게임을 학습하는 데 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):  # 연속성을 위한 하위 클래스입니다.\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
        "        self.batch_size = 32\n",
        "        self.prev_info = None\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        입력값:\n",
        "        state (``LazyFrame``),\n",
        "        next_state (``LazyFrame``),\n",
        "        action (``int``),\n",
        "        reward (``float``),\n",
        "        done(``bool``))\n",
        "        \"\"\"\n",
        "\n",
        "        # 리워드 계산\n",
        "        reward = self.calculate_reward(reward, info, done, self.prev_info)\n",
        "        self.prev_info = info.copy()\n",
        "        \n",
        "        # 기존 캐시 로직\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
        "        action = torch.tensor([action], dtype=torch.int64)\n",
        "        reward = torch.tensor([reward], dtype=torch.float32)\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        # self.memory.append((state, next_state, action, reward, done,))\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        메모리에서 일련의 경험들을 검색합니다.\n",
        "        \"\"\"\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "학습하기(Learn)\n",
        "===============\n",
        "\n",
        "마리오는 [DDQN 알고리즘](https://arxiv.org/pdf/1509.06461) 을\n",
        "사용합니다. DDQN 두개의 ConvNets ( $Q_{online}$ 과 $Q_{target}$ ) 을\n",
        "사용하고, 독립적으로 최적의 행동-가치 함수에 근사시키려고 합니다.\n",
        "\n",
        "구현을 할 때, 특징 생성기에서 `특징들` 을 $Q_{online}$ 와 $Q_{target}$\n",
        "에 공유합니다. 그러나 각각의 FC 분류기는 가지고 있도록 설계합니다.\n",
        "$\\theta_{target}$ ($Q_{target}$ 의 매개변수 값) 는 역전파에 의해 값이\n",
        "업데이트 되지 않도록 고정되었습니다. 대신, $\\theta_{online}$ 와\n",
        "주기적으로 동기화를 진행합니다. 이것에 대해서는 추후에 다루도록\n",
        "하겠습니다.)\n",
        "\n",
        "신경망\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"작은 CNN 구조\n",
        "  입력 -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> 출력\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = self.__build_cnn(c, output_dim)\n",
        "\n",
        "        self.target = self.__build_cnn(c, output_dim)\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "        # Q_target 매개변수 값은 고정시킵니다.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model=\"online\"):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)\n",
        "\n",
        "    def __build_cnn(self, c, output_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TD 추정 & TD 목표값\n",
        "===================\n",
        "\n",
        "학습을 하는데 두 가지 값들이 포함됩니다.\n",
        "\n",
        "**TD 추정** - 주어진 상태 $s$ 에서 최적의 예측 $Q^*$.\n",
        "\n",
        "$${TD}_e = Q_{online}^*(s,a)$$\n",
        "\n",
        "**TD 목표** - 현재의 포상과 다음상태 $s'$ 에서 추정된 $Q^*$ 의 합.\n",
        "\n",
        "$$a' = argmax_{a} Q_{online}(s', a)$$\n",
        "\n",
        "$${TD}_t = r + \\gamma Q_{target}^*(s',a')$$\n",
        "\n",
        "다음 행동 $a'$ 가 어떨지 모르기 때문에 다음 상태 $s'$ 에서 $Q_{online}$\n",
        "값이 최대가 되도록 하는 행동 $a'$ 를 사용합니다.\n",
        "\n",
        "여기에서 변화도 계산을 비활성화하기 위해 `td_target()` 에서\n",
        "[\\@torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad)\n",
        "데코레이터(decorator)를 사용합니다. ($\\theta_{target}$ 의 역전파 계산이\n",
        "필요로 하지 않기 때문입니다.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.gamma = 0.9\n",
        "        self.reward_scale = 0.1  # 보상 스케일링 파라미터\n",
        "        self.q_scale = 1.0      # Q값 스케일링 파라미터\n",
        "        \n",
        "        # 보상 정규화를 위한 통계\n",
        "        self.reward_mean = 0\n",
        "        self.reward_std = 1\n",
        "        self.reward_count = 0\n",
        "        \n",
        "        # Q값 정규화를 위한 통계\n",
        "        self.q_mean = 0\n",
        "        self.q_std = 1\n",
        "        self.q_count = 0\n",
        "    \n",
        "    def update_statistics(self, value, is_reward=True):\n",
        "        \"\"\"온라인 통계 업데이트\"\"\"\n",
        "        if is_reward:\n",
        "            self.reward_count += 1\n",
        "            delta = value - self.reward_mean\n",
        "            self.reward_mean += delta / self.reward_count\n",
        "            delta2 = value - self.reward_mean\n",
        "            self.reward_std += delta * delta2\n",
        "        else:\n",
        "            self.q_count += 1\n",
        "            delta = value - self.q_mean\n",
        "            self.q_mean += delta / self.q_count\n",
        "            delta2 = value - self.q_mean\n",
        "            self.q_std += delta * delta2\n",
        "\n",
        "    def normalize_value(self, value, is_reward=True):\n",
        "        \"\"\"값 정규화\"\"\"\n",
        "        if is_reward:\n",
        "            return (value - self.reward_mean) / (self.reward_std + 1e-8)\n",
        "        else:\n",
        "            return (value - self.q_mean) / (self.q_std + 1e-8)\n",
        "\n",
        "    def calculate_reward(self, reward, info, done, prev_info=None):\n",
        "        \"\"\"보상 계산 및 스케일링\"\"\"\n",
        "        total_reward = reward * self.reward_scale\n",
        "        \n",
        "        if info[\"flag_get\"]:    # 정복\n",
        "            total_reward += 10000\n",
        "        if prev_info and info[\"coins\"] > prev_info[\"coins\"]:    # 코인 획득\n",
        "            total_reward += 1000\n",
        "        if not done:    # 진행도\n",
        "            total_reward += 1.0\n",
        "        if prev_info and info[\"x_pos\"] > prev_info[\"x_pos\"]:    # 진행도 보너스\n",
        "            total_reward += 10\n",
        "        if prev_info and info[\"life\"] < prev_info[\"life\"]:    # 생명 감소시 200점 패널티\n",
        "            total_reward -= 200\n",
        "        if prev_info and info[\"y_pos\"] < prev_info[\"y_pos\"]:  # 위로 올라갈 때\n",
        "            total_reward += 5\n",
        "        if prev_info and info[\"x_pos\"] - prev_info[\"x_pos\"] > 5:  # 빠른 이동\n",
        "            total_reward += 15\n",
        "        # 보상 정규화\n",
        "        normalized_reward = self.normalize_value(total_reward, is_reward=True)\n",
        "        self.update_statistics(total_reward, is_reward=True)\n",
        "        \n",
        "        return normalized_reward\n",
        "\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        \n",
        "        # 정규화된 값으로 TD 타겟 계산\n",
        "        normalized_Q = self.normalize_value(next_Q, is_reward=False)\n",
        "\n",
        "        #td_target = (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "        td_target = (reward + (1 - done.float()) * self.gamma * normalized_Q).float()\n",
        "        \n",
        "        return td_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 업데이트\n",
        "=============\n",
        "\n",
        "마리오가 재생 버퍼에서 입력을 샘플링할 때, $TD_t$ 와 $TD_e$ 를\n",
        "계산합니다. 그리고 이 손실을 이용하여 $Q_{online}$ 역전파하여 매개변수\n",
        "$\\theta_{online}$ 를 업데이트합니다. ($\\alpha$ 는 `optimizer` 에\n",
        "전달되는 학습률 `lr` 입니다.)\n",
        "\n",
        "$$\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)$$\n",
        "\n",
        "$\\theta_{target}$ 은 역전파를 통해 업데이트 되지 않습니다. 대신,\n",
        "주기적으로 $\\theta_{online}$ 의 값을 $\\theta_{target}$ 로 복사합니다.\n",
        "\n",
        "$$\\theta_{target} \\leftarrow \\theta_{online}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "체크포인트 저장\n",
        "===============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모든 기능을 합치기\n",
        "==================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.burnin = 1e3  # 학습을 진행하기 전 최소한의 경험값. 1e4 -> 1e3 학습을 더 빠르게 시작.\n",
        "        self.learn_every = 1  # Q_online 업데이트 사이의 경험 횟수. 매번 학습 3 -> 1\n",
        "        self.sync_every = 1e3  # Q_target과 Q_online sync 사이의 경험 수 1e4 -> 1e3\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # 메모리로부터 샘플링을 합니다.\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # TD 추정값을 가져옵니다.\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # TD 목표값을 가져옵니다.\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # 실시간 Q(Q_online)을 통해 역전파 손실을 계산합니다.\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "기록하기\n",
        "========\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir, model=None, device=None):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        self.writer = SummaryWriter(os.path.join(save_dir, \"tensorboard\"))\n",
        "\n",
        "        # model 기록\n",
        "        if model is None:\n",
        "            raise ValueError(\"model is required\")\n",
        "        \n",
        "        dummy_input = torch.zeros((1, 4, 84, 84)).to(device)\n",
        "        self.writer.add_graph(model, dummy_input)  # 모델 구조 기록\n",
        "        # 모델 파라미터 수 계산 및 기록\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        self.writer.add_text('Model/Parameters', f'Total Parameters: {total_params:,}\\nTrainable Parameters: {trainable_params:,}')\n",
        "        \n",
        "        # 각 레이어의 파라미터 수 기록\n",
        "        for name, param in model.named_parameters():\n",
        "            self.writer.add_text('Model/Layers', f'{name}: {param.numel():,} parameters')\n",
        "        \n",
        "\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # 지표(Metric)와 관련된 리스트입니다.\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # 모든 record() 함수를 호출한 후 이동 평균(Moving average)을 계산합니다.\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # 현재 에피스드에 대한 지표를 기록합니다.\n",
        "        self.init_episode()\n",
        "\n",
        "        # 시간에 대한 기록입니다.\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"에피스드의 끝을 표시합니다.\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step, model=None):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        \n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        # tensorboard에 기록\n",
        "        self.writer.add_scalar('Metrics/Mean Reward', mean_ep_reward, episode)\n",
        "        self.writer.add_scalar('Metrics/Mean Length', mean_ep_length, episode)\n",
        "        self.writer.add_scalar('Metrics/Mean Loss', mean_ep_loss, episode)\n",
        "        self.writer.add_scalar('Metrics/Mean Q Value', mean_ep_q, episode)\n",
        "        self.writer.add_scalar('Metrics/Epsilon', epsilon, episode)\n",
        "        self.writer.add_scalar('Metrics/Step', step, episode)\n",
        "\n",
        "        # 모델 파라미터의 그래디언트와 가중치 분포 기록\n",
        "        if model is not None:\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    self.writer.add_histogram(f'Gradients/{name}', param.grad, episode)\n",
        "                self.writer.add_histogram(f'Weights/{name}', param.data, episode)    \n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            self.writer.add_figure(f'Plots/{metric}', plt.gcf(), episode)\n",
        "    \n",
        "    def close(self):\n",
        "        self.writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "게임을 실행시켜봅시다!\n",
        "======================\n",
        "\n",
        "이번 예제에서는 40개의 에피소드에 대해 학습 루프를 실행시켰습니다.하지만\n",
        "마리오가 진정으로 세계를 학습하기 위해서는 적어도 40000개의 에피소드에\n",
        "대해 학습을 시킬 것을 제안합니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: False\n",
            "Using MPS: True\n",
            "Episode 0 - Step 43 - Epsilon 0.9999892500564358 - Mean Reward 226.0 - Mean Length 43.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.9 - Time 2025-05-26T17:16:34\n",
            "Episode 5 - Step 655 - Epsilon 0.9998362633858103 - Mean Reward 517.167 - Mean Length 109.167 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 8.376 - Time 2025-05-26T17:16:42\n",
            "Episode 10 - Step 929 - Epsilon 0.9997677769388855 - Mean Reward 419.364 - Mean Length 84.455 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 4.226 - Time 2025-05-26T17:16:46\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m mario\u001b[38;5;241m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 배우기\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 기록하기\u001b[39;00m\n\u001b[1;32m     34\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_step(reward, loss, q)\n",
            "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mMario.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 메모리로부터 샘플링을 합니다.\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m state, next_state, action, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# TD 추정값을 가져옵니다.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m td_est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_estimate(state, action)\n",
            "Cell \u001b[0;32mIn[21], line 43\u001b[0m, in \u001b[0;36mMario.recall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrecall\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    메모리에서 일련의 경험들을 검색합니다.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     state, next_state, action, reward, done \u001b[38;5;241m=\u001b[39m (batch\u001b[38;5;241m.\u001b[39mget(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, next_state, action\u001b[38;5;241m.\u001b[39msqueeze(), reward\u001b[38;5;241m.\u001b[39msqueeze(), done\u001b[38;5;241m.\u001b[39msqueeze()\n",
            "File \u001b[0;32m~/Documents/WorkSpace/RL-game/.venv/lib/python3.10/site-packages/tensordict/base.py:13615\u001b[0m, in \u001b[0;36mTensorDictBase.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m  13608\u001b[0m     result\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m  13609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m  13610\u001b[0m     device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m  13611\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sub_non_blocking\n\u001b[1;32m  13612\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m non_blocking\n\u001b[1;32m  13613\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m  13614\u001b[0m ):\n\u001b[0;32m> 13615\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sync_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  13616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/Documents/WorkSpace/RL-game/.venv/lib/python3.10/site-packages/tensordict/base.py:13745\u001b[0m, in \u001b[0;36mTensorDictBase._sync_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13743\u001b[0m mps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m  13744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m> 13745\u001b[0m     \u001b[43mmps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/WorkSpace/RL-game/.venv/lib/python3.10/site-packages/torch/mps/__init__.py:33\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msynchronize\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Waits for all kernels in all streams on a MPS device to complete.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mps_deviceSynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "use_mps = torch.backends.mps.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print(f\"Using MPS: {use_mps}\")\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir, model = mario.net, device = mario.device)\n",
        "\n",
        "episodes = 10000\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # 게임을 실행시켜봅시다!\n",
        "    while True:\n",
        "\n",
        "        # 현재 상태에서 에이전트 실행하기\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # 에이전트가 액션 수행하기\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # 기억하기\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # 배우기\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # 기록하기\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # 상태 업데이트하기\n",
        "        state = next_state\n",
        "\n",
        "        # 게임이 끝났는지 확인하기\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if (e % 5 == 0) or (e == episodes - 1):\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step, model=mario.net)\n",
        "logger.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "## 위의 셀을 실행하면서 새로운 terminal을 열고 아래의 명령어로 tenosrboard 실행\n",
        "\n",
        "tensorboard --logdir=<log path>\n",
        "# e.g. tensorboard --logdir=checkpoints/2025-05-16T10-12044"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.multiprocessing as mp\n",
        "from torch.multiprocessing import Process, Queue\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class ParallelEnv:\n",
        "    def __init__(self, num_envs=4):\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = []\n",
        "        self.queues = []\n",
        "        \n",
        "        for _ in range(num_envs):\n",
        "            env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb-array', apply_api_compatibility=True)\n",
        "            env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "            env = SkipFrame(env, skip=4)\n",
        "            env = GrayScaleObservation(env)\n",
        "            env = ResizeObservation(env, shape=84)\n",
        "            env = FrameStack(env, num_stack=4)\n",
        "            self.envs.append(env)\n",
        "            self.queues.append(Queue())\n",
        "    \n",
        "    def reset(self):\n",
        "        states = []\n",
        "        for env in self.envs:\n",
        "            state = env.reset()\n",
        "            states.append(state)\n",
        "        return states\n",
        "    \n",
        "    def step(self, actions):\n",
        "        states = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "        infos = []\n",
        "        \n",
        "        for i, (env, action) in enumerate(zip(self.envs, actions)):\n",
        "            state, reward, done, trunc, info = env.step(action)\n",
        "            states.append(state)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            infos.append(info)\n",
        "            \n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                states[i] = state\n",
        "        \n",
        "        return states, rewards, dones, infos\n",
        "\n",
        "def train_worker(worker_id, shared_memory, num_episodes):\n",
        "    # 각 워커별 환경 초기화\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb-array', apply_api_compatibility=True)\n",
        "    env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "    env = SkipFrame(env, skip=4)\n",
        "    env = GrayScaleObservation(env)\n",
        "    env = ResizeObservation(env, shape=84)\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "    \n",
        "    # 에이전트 초기화\n",
        "    mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=Path(\"checkpoints\"))\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        \n",
        "        while True:\n",
        "            action = mario.act(state)\n",
        "            next_state, reward, done, trunc, info = env.step(action)\n",
        "            \n",
        "            # 경험 저장\n",
        "            mario.cache(state, next_state, action, reward, done)\n",
        "            \n",
        "            # 학습\n",
        "            q, loss = mario.learn()\n",
        "            \n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            \n",
        "            if done or info[\"flag_get\"]:\n",
        "                break\n",
        "        \n",
        "        # 공유 메모리에 결과 저장\n",
        "        shared_memory.put({\n",
        "            'worker_id': worker_id,\n",
        "            'episode': episode,\n",
        "            'reward': episode_reward\n",
        "        })\n",
        "\n",
        "def main():\n",
        "    num_workers = 4  # 병렬로 실행할 워커 수\n",
        "    num_episodes = 10000  # 각 워커당 에피소드 수\n",
        "    \n",
        "    # 공유 메모리 초기화\n",
        "    shared_memory = Queue()\n",
        "    \n",
        "    # 워커 프로세스 생성\n",
        "    processes = []\n",
        "    for i in range(num_workers):\n",
        "        p = Process(target=train_worker, args=(i, shared_memory, num_episodes))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "    \n",
        "    # 메인 에이전트 초기화\n",
        "    mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=Path(\"checkpoints\"))\n",
        "    logger = MetricLogger(save_dir=Path(\"checkpoints\"))\n",
        "    \n",
        "    # 결과 수집 및 로깅\n",
        "    total_episodes = 0\n",
        "    while total_episodes < num_workers * num_episodes:\n",
        "        result = shared_memory.get()\n",
        "        logger.log_episode()\n",
        "        logger.record(\n",
        "            episode=result['episode'],\n",
        "            epsilon=mario.exploration_rate,\n",
        "            step=mario.curr_step\n",
        "        )\n",
        "        total_episodes += 1\n",
        "    \n",
        "    # 프로세스 종료\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    \n",
        "    logger.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.set_start_method('spawn')  # Windows 호환성을 위해 필요\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "결론\n",
        "====\n",
        "\n",
        "이 튜토리얼에서는 PyTorch를 사용하여 게임 플레이 AI를 훈련하는 방법을\n",
        "살펴보았습니다. [OpenAI gym](https://gym.openai.com/) 에 있는 어떤\n",
        "게임이든 동일한 방법으로 AI를 훈련시키고 게임을 진행할 수 있습니다. 이\n",
        "튜토리얼이 도움이 되었기를 바라며, [Github\n",
        "저장소](https://github.com/yuansongFeng/MadMario/) 에서 편하게\n",
        "저자들에게 연락을 하셔도 됩니다!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
